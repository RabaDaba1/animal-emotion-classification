{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f60bba3",
   "metadata": {},
   "source": [
    "# Fine-tuning MobileNetV3 for Pet Emotion Classification\n",
    "\n",
    "This notebook fine-tunes a pretrained MobileNetV3 model on two pet emotion datasets to classify emotions: **happy**, **sad**, and **angry**.\n",
    "\n",
    "## Datasets:\n",
    "1. Dog Emotion Dataset (4000 images)\n",
    "2. Pet's Facial Expression Dataset (1000 images)\n",
    "\n",
    "We'll combine both datasets and focus only on the three target emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87833b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install timm torch torchvision pandas pillow scikit-learn matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dda700c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import timm\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d75318",
   "metadata": {},
   "source": [
    "## Data Loading and Preparation\n",
    "\n",
    "First, let's load both datasets and combine them for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048fe053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset paths (adjust these based on your Kaggle input paths)\n",
    "DOG_EMOTION_PATH = \"/kaggle/input/dog-emotion/Dog Emotion\"\n",
    "PET_EXPRESSION_PATH = \"/kaggle/input/pets-facial-expression-image-dataset\"\n",
    "\n",
    "# Create a combined dataset directory\n",
    "COMBINED_DATA_PATH = \"/kaggle/working/combined_pet_emotions\"\n",
    "os.makedirs(COMBINED_DATA_PATH, exist_ok=True)\n",
    "\n",
    "# Create directories for our target emotions\n",
    "emotions = [\"happy\", \"sad\", \"angry\"]\n",
    "for emotion in emotions:\n",
    "    os.makedirs(os.path.join(COMBINED_DATA_PATH, emotion), exist_ok=True)\n",
    "\n",
    "print(\"Created directories for combined dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8f0d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to copy images to combined dataset\n",
    "def copy_images_to_combined_dataset():\n",
    "    image_count = 0\n",
    "\n",
    "    # Process Dog Emotion dataset\n",
    "    print(\"Processing Dog Emotion dataset...\")\n",
    "    for emotion in emotions:\n",
    "        source_dir = os.path.join(DOG_EMOTION_PATH, emotion)\n",
    "        target_dir = os.path.join(COMBINED_DATA_PATH, emotion)\n",
    "\n",
    "        if os.path.exists(source_dir):\n",
    "            for img_file in os.listdir(source_dir):\n",
    "                if img_file.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                    source_path = os.path.join(source_dir, img_file)\n",
    "                    target_path = os.path.join(target_dir, f\"dog_{img_file}\")\n",
    "                    shutil.copy2(source_path, target_path)\n",
    "                    image_count += 1\n",
    "\n",
    "    # Process Pet Expression dataset\n",
    "    print(\"Processing Pet Expression dataset...\")\n",
    "    emotion_mapping = {\"happy\": \"happy\", \"Sad\": \"sad\", \"Angry\": \"angry\"}\n",
    "\n",
    "    for source_emotion, target_emotion in emotion_mapping.items():\n",
    "        source_dir = os.path.join(PET_EXPRESSION_PATH, source_emotion)\n",
    "        target_dir = os.path.join(COMBINED_DATA_PATH, target_emotion)\n",
    "\n",
    "        if os.path.exists(source_dir):\n",
    "            for img_file in os.listdir(source_dir):\n",
    "                if img_file.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                    source_path = os.path.join(source_dir, img_file)\n",
    "                    target_path = os.path.join(target_dir, f\"pet_{img_file}\")\n",
    "                    shutil.copy2(source_path, target_path)\n",
    "                    image_count += 1\n",
    "\n",
    "    return image_count\n",
    "\n",
    "\n",
    "# Copy all images\n",
    "total_images = copy_images_to_combined_dataset()\n",
    "print(f\"Total images copied: {total_images}\")\n",
    "\n",
    "# Check distribution\n",
    "for emotion in emotions:\n",
    "    emotion_dir = os.path.join(COMBINED_DATA_PATH, emotion)\n",
    "    count = len(\n",
    "        [\n",
    "            f\n",
    "            for f in os.listdir(emotion_dir)\n",
    "            if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
    "        ]\n",
    "    )\n",
    "    print(f\"{emotion}: {count} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2824be3e",
   "metadata": {},
   "source": [
    "## Data Transforms and Dataset Setup\n",
    "\n",
    "Define data augmentation and normalization transforms for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bb473f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms\n",
    "train_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(degrees=15),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create datasets\n",
    "full_dataset = ImageFolder(root=COMBINED_DATA_PATH, transform=train_transforms)\n",
    "print(f\"Total dataset size: {len(full_dataset)}\")\n",
    "print(f\"Classes: {full_dataset.classes}\")\n",
    "print(f\"Class to index mapping: {full_dataset.class_to_idx}\")\n",
    "\n",
    "# Split dataset (80% train, 20% validation)\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "# Create validation dataset with different transforms\n",
    "val_dataset.dataset.transform = val_transforms\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f422c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2338ac",
   "metadata": {},
   "source": [
    "## Model Setup\n",
    "\n",
    "Load the pretrained MobileNetV3 model and modify it for our 3-class classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30db401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained MobileNetV3 model\n",
    "model = timm.create_model(\"mobilenetv3_large_100\", pretrained=True, num_classes=3)\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "\n",
    "# Print model summary\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d451c8fb",
   "metadata": {},
   "source": [
    "## Training Setup\n",
    "\n",
    "Define loss function, optimizer, and training configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e56af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "weight_decay = 1e-4\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "print(\"Training setup complete!\")\n",
    "print(f\"Number of epochs: {num_epochs}\")\n",
    "print(f\"Learning rate: {learning_rate}\")\n",
    "print(f\"Batch size: {batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115db972",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "Train the model with validation monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb83429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_model(\n",
    "    model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs\n",
    "):\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            if (batch_idx + 1) % 50 == 0:\n",
    "                print(\n",
    "                    f\"Batch [{batch_idx + 1}/{len(train_loader)}], Loss: {loss.item():.4f}\"\n",
    "                )\n",
    "\n",
    "        train_acc = 100 * train_correct / train_total\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "\n",
    "        # Store metrics\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_state = model.state_dict().copy()\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "        print(f\"Best Val Acc: {best_val_acc:.2f}%\")\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    return {\n",
    "        \"train_losses\": train_losses,\n",
    "        \"train_accuracies\": train_accuracies,\n",
    "        \"val_losses\": val_losses,\n",
    "        \"val_accuracies\": val_accuracies,\n",
    "        \"best_model_state\": best_model_state,\n",
    "        \"best_val_acc\": best_val_acc,\n",
    "    }\n",
    "\n",
    "\n",
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "training_results = train_model(\n",
    "    model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs\n",
    ")\n",
    "print(\n",
    "    f\"\\nTraining completed! Best validation accuracy: {training_results['best_val_acc']:.2f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa14db6",
   "metadata": {},
   "source": [
    "## Training Results Visualization\n",
    "\n",
    "Plot training and validation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d58b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot losses\n",
    "ax1.plot(training_results[\"train_losses\"], label=\"Train Loss\", color=\"blue\")\n",
    "ax1.plot(training_results[\"val_losses\"], label=\"Validation Loss\", color=\"red\")\n",
    "ax1.set_title(\"Training and Validation Loss\")\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Plot accuracies\n",
    "ax2.plot(training_results[\"train_accuracies\"], label=\"Train Accuracy\", color=\"blue\")\n",
    "ax2.plot(training_results[\"val_accuracies\"], label=\"Validation Accuracy\", color=\"red\")\n",
    "ax2.set_title(\"Training and Validation Accuracy\")\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax2.set_ylabel(\"Accuracy (%)\")\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final metrics\n",
    "print(f\"Final Train Accuracy: {training_results['train_accuracies'][-1]:.2f}%\")\n",
    "print(f\"Final Validation Accuracy: {training_results['val_accuracies'][-1]:.2f}%\")\n",
    "print(f\"Best Validation Accuracy: {training_results['best_val_acc']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f11c54",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Load the best model and evaluate on validation set with detailed metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40129b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(training_results[\"best_model_state\"])\n",
    "model.eval()\n",
    "\n",
    "# Evaluate on validation set\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Convert to numpy arrays\n",
    "all_predictions = np.array(all_predictions)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# Classification report\n",
    "class_names = [\"angry\", \"happy\", \"sad\"]\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_labels, all_predictions, target_names=class_names))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_predictions)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=class_names,\n",
    "    yticklabels=class_names,\n",
    ")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee80f12",
   "metadata": {},
   "source": [
    "## Save the Model\n",
    "\n",
    "Save the trained model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9a308b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model directory\n",
    "model_dir = \"/kaggle/working/pet_emotion_model\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the complete model\n",
    "model_path = os.path.join(model_dir, \"mobilenetv3_pet_emotion_classifier.pth\")\n",
    "torch.save(\n",
    "    {\n",
    "        \"model_state_dict\": training_results[\"best_model_state\"],\n",
    "        \"model_architecture\": \"mobilenetv3_large_100\",\n",
    "        \"num_classes\": 3,\n",
    "        \"class_names\": class_names,\n",
    "        \"class_to_idx\": full_dataset.class_to_idx,\n",
    "        \"best_val_acc\": training_results[\"best_val_acc\"],\n",
    "        \"train_transforms\": str(train_transforms),\n",
    "        \"val_transforms\": str(val_transforms),\n",
    "    },\n",
    "    model_path,\n",
    ")\n",
    "\n",
    "# Save model configuration\n",
    "config_path = os.path.join(model_dir, \"model_config.txt\")\n",
    "with open(config_path, \"w\") as f:\n",
    "    f.write(f\"Model: MobileNetV3 Large 100\\n\")\n",
    "    f.write(f\"Number of classes: 3\\n\")\n",
    "    f.write(f\"Class names: {class_names}\\n\")\n",
    "    f.write(f\"Class to index mapping: {full_dataset.class_to_idx}\\n\")\n",
    "    f.write(f\"Best validation accuracy: {training_results['best_val_acc']:.2f}%\\n\")\n",
    "    f.write(f\"Total training images: {len(train_dataset)}\\n\")\n",
    "    f.write(f\"Total validation images: {len(val_dataset)}\\n\")\n",
    "    f.write(f\"Training epochs: {num_epochs}\\n\")\n",
    "    f.write(f\"Batch size: {batch_size}\\n\")\n",
    "    f.write(f\"Learning rate: {learning_rate}\\n\")\n",
    "\n",
    "print(f\"Model saved successfully to: {model_path}\")\n",
    "print(f\"Model configuration saved to: {config_path}\")\n",
    "\n",
    "# Show saved files\n",
    "print(\"\\nSaved files:\")\n",
    "for file in os.listdir(model_dir):\n",
    "    file_path = os.path.join(model_dir, file)\n",
    "    file_size = os.path.getsize(file_path) / (1024 * 1024)  # Size in MB\n",
    "    print(f\"  {file}: {file_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81045b09",
   "metadata": {},
   "source": [
    "## Model Loading Example\n",
    "\n",
    "Example of how to load and use the saved model for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d650cf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load the saved model for inference\n",
    "def load_trained_model(model_path):\n",
    "    \"\"\"Load the trained model for inference\"\"\"\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "\n",
    "    # Create model architecture\n",
    "    model = timm.create_model(\"mobilenetv3_large_100\", pretrained=False, num_classes=3)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    return model, checkpoint\n",
    "\n",
    "\n",
    "# Load the model\n",
    "loaded_model, checkpoint = load_trained_model(model_path)\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"Best validation accuracy: {checkpoint['best_val_acc']:.2f}%\")\n",
    "print(f\"Class names: {checkpoint['class_names']}\")\n",
    "\n",
    "\n",
    "# Example inference function\n",
    "def predict_emotion(model, image_path, transform):\n",
    "    \"\"\"Predict emotion for a single image\"\"\"\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image_tensor)\n",
    "        probabilities = torch.nn.functional.softmax(outputs[0], dim=0)\n",
    "        predicted_class = torch.argmax(outputs, dim=1).item()\n",
    "\n",
    "    return predicted_class, probabilities.cpu().numpy()\n",
    "\n",
    "\n",
    "print(\"\\nModel is ready for inference!\")\n",
    "print(\"Use the predict_emotion function to classify new pet images.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561aefdd",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Training Results:\n",
    "- **Model**: MobileNetV3 Large 100\n",
    "- **Classes**: angry, happy, sad\n",
    "- **Best Validation Accuracy**: {:.2f}%\n",
    "- **Total Images**: Combined from Dog Emotion and Pet Expression datasets\n",
    "- **Training Strategy**: Transfer learning with data augmentation\n",
    "\n",
    "### Files Saved:\n",
    "- `mobilenetv3_pet_emotion_classifier.pth`: Complete model checkpoint\n",
    "- `model_config.txt`: Model configuration and metadata\n",
    "\n",
    "The model is now ready for deployment and can classify pet emotions into three categories: angry, happy, and sad."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
